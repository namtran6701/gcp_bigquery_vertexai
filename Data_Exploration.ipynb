{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration for tunning a foundation model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project evnironment set up:\n",
    "- Since wer are running the code locally, there is a few libraries and steps that we need to complete to get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed library \n",
    "from utils import authenticate\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import vertexai \n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide credential to to access google cloud platform\n",
    "PROJECT_ID = \"de-tunning-foundation-model\"\n",
    "credentials = service_account.Credentials.from_service_account_file('credentials.json')\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to initialize Vertex AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION,\n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import big query to use as our data warehouse. First we will need to initialize the client to start interacting with the data warehouse, next, send SQL and retrieve data into the notebook\n",
    "- This dataset include questions, answer, and metadata related to stack overflow questions. With this dataset, there are tables with data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID,\n",
    "                            credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TABLES = \"\"\"\n",
    "SELECT\n",
    "  table_name\n",
    "FROM\n",
    "  `bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bq_client.query(QUERY_TABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will print out all the tables in the stackoverflow directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts_answers\n",
      "users\n",
      "posts_orphaned_tag_wiki\n",
      "posts_tag_wiki\n",
      "stackoverflow_posts\n",
      "posts_questions\n",
      "comments\n",
      "posts_tag_wiki_excerpt\n",
      "posts_wiki_placeholder\n",
      "posts_privilege_wiki\n",
      "post_history\n",
      "badges\n",
      "post_links\n",
      "tags\n",
      "votes\n",
      "posts_moderator_nomination\n"
     ]
    }
   ],
   "source": [
    "for row in query_job:\n",
    "    for value in row.values():\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we will fetch some data from the data warehouse and store it in pandas df for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 3 questions from the post_questions data warehouse \n",
    "INSPECT_QUERY = \"\"\"\n",
    "SELECT * \n",
    "FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "LIMIT 3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the dataset in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the query\n",
    "query_job = bq_client.query(INSPECT_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the results of the query --> create an arrow table (which is part of Apache Framework) --> which goes into a Pandas dataframe.\n",
    "This allows for data to be in a format which is easier to read and explore with Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Arrow is an open-source project that provides a standard for in-memory data representation. It's designed to enable efficient data processing and communication between systems. An Arrow table is essentially a collection of Arrow arrays organized into columns. These arrays are stored in a columnar memory format, which makes operations on large datasets much faster compared to traditional row-based storage. This is especially beneficial in analytics and data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>community_owned_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>last_editor_display_name</th>\n",
       "      <th>last_editor_user_id</th>\n",
       "      <th>owner_display_name</th>\n",
       "      <th>owner_user_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>post_type_id</th>\n",
       "      <th>score</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73210679</td>\n",
       "      <td>az acr login raises DOCKER_COMMAND_ERROR with ...</td>\n",
       "      <td>&lt;p&gt;Windows 11 with wsl2 ubuntu-22.04.&lt;/p&gt;\\n&lt;p&gt;...</td>\n",
       "      <td>73247188.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-08-02 16:16:31.810000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-05 09:00:14.693000+00:00</td>\n",
       "      <td>2022-08-02 16:32:13.700000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>11226740</td>\n",
       "      <td>None</td>\n",
       "      <td>11226740</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>azure-container-registry|docker-daemon</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73284406</td>\n",
       "      <td>Run Azure log query from the command line with...</td>\n",
       "      <td>&lt;p&gt;I am trying to get the Azure log query data...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-08-08 21:53:16.703000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-10 04:49:20.527000+00:00</td>\n",
       "      <td>2022-08-09 08:12:13.920000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2395282</td>\n",
       "      <td>None</td>\n",
       "      <td>19123691</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>azure|azure-devops|azure-application-insights|...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73250763</td>\n",
       "      <td>Error CS0246: The type or namespace name 'Stre...</td>\n",
       "      <td>&lt;p&gt;I have these errors when trying to write th...</td>\n",
       "      <td>73251390.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-08-05 13:43:25.850000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-08-05 16:40:00.610000+00:00</td>\n",
       "      <td>2022-08-05 16:40:00.610000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>17654458</td>\n",
       "      <td>None</td>\n",
       "      <td>17654458</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>c#|unity3d</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  73210679  az acr login raises DOCKER_COMMAND_ERROR with ...   \n",
       "1  73284406  Run Azure log query from the command line with...   \n",
       "2  73250763  Error CS0246: The type or namespace name 'Stre...   \n",
       "\n",
       "                                                body  accepted_answer_id  \\\n",
       "0  <p>Windows 11 with wsl2 ubuntu-22.04.</p>\\n<p>...          73247188.0   \n",
       "1  <p>I am trying to get the Azure log query data...                 NaN   \n",
       "2  <p>I have these errors when trying to write th...          73251390.0   \n",
       "\n",
       "   answer_count  comment_count community_owned_date  \\\n",
       "0             1              0                  NaT   \n",
       "1             2              0                  NaT   \n",
       "2             1              0                  NaT   \n",
       "\n",
       "                     creation_date  favorite_count  \\\n",
       "0 2022-08-02 16:16:31.810000+00:00             NaN   \n",
       "1 2022-08-08 21:53:16.703000+00:00             NaN   \n",
       "2 2022-08-05 13:43:25.850000+00:00             NaN   \n",
       "\n",
       "                last_activity_date                   last_edit_date  \\\n",
       "0 2022-08-05 09:00:14.693000+00:00 2022-08-02 16:32:13.700000+00:00   \n",
       "1 2022-08-10 04:49:20.527000+00:00 2022-08-09 08:12:13.920000+00:00   \n",
       "2 2022-08-05 16:40:00.610000+00:00 2022-08-05 16:40:00.610000+00:00   \n",
       "\n",
       "  last_editor_display_name  last_editor_user_id owner_display_name  \\\n",
       "0                     None             11226740               None   \n",
       "1                     None              2395282               None   \n",
       "2                     None             17654458               None   \n",
       "\n",
       "   owner_user_id parent_id  post_type_id  score  \\\n",
       "0       11226740      None             1      0   \n",
       "1       19123691      None             1      0   \n",
       "2       17654458      None             1      3   \n",
       "\n",
       "                                                tags  view_count  \n",
       "0             azure-container-registry|docker-daemon         256  \n",
       "1  azure|azure-devops|azure-application-insights|...         256  \n",
       "2                                         c#|unity3d         512  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_overflow_df = query_job.result()\\\n",
    "    .to_arrow()\\\n",
    "        .to_pandas()\n",
    "        \n",
    "stack_overflow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, large datasets for LLMs often don't fit into memory. \n",
    "\n",
    "We can see this right below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ALL = \"\"\"\n",
    "select \n",
    "* \n",
    "from `bigquery-public-data.stackoverflow.posts_questions` q\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bq_client.query(QUERY_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe is too large to load into memeory. 403 Response too large to return. Consider specifying a destination table in your job configuration. For more details, see https://cloud.google.com/bigquery/troubleshooting-errors\n",
      "\n",
      "Location: US\n",
      "Job ID: 1c575932-ec23-49f9-bb5d-5ea6414d5a99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    stack_overflow_df = query_job.result()\\\n",
    "        .to_arrow()\\\n",
    "            .to_pandas()    \n",
    "except Exception as e:\n",
    "    print('The dataframe is too large to load into memeory.', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the query is too large to be fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining Tables and Query Optimization\n",
    "\n",
    "- When working with (large) data, query optimizing is needed in order to save time and resources.\n",
    "- Select questions as `input_text` (column 1), answers as `output_text` (column 2).\n",
    "- Take the questions from `posts_questions` and answers from `posts_answers`.\n",
    "- Join the questions and their corresponding accepted answers based on their same `unique ID`.\n",
    "- Making sure the question is about `Python`, and that it `has an answer`. And the date the question was posted is on or after `2020-01-01`\n",
    "- Limit as 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"\"\" \n",
    "select \n",
    "concat(q.title, q.body) as input_text,\n",
    "a.body as output_text\n",
    "from `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "join `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "on q.accepted_answer_id = a.id\n",
    "where \n",
    "q.accepted_answer_id is not null\n",
    "and q.tags like '%python%'\n",
    "and a.creation_date > '2020-01-01'\n",
    "limit 10000\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = bq_client.query(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>igraph: get dict with nodes and corresponding ...</td>\n",
       "      <td>&lt;p&gt;You can recover the original networkx node ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first coordinates never work, but the foll...</td>\n",
       "      <td>&lt;p&gt;Those first four lines:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;b =...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is there a performance deficit not splitting e...</td>\n",
       "      <td>&lt;p&gt;Depends if your &lt;code&gt;function_one&lt;/code&gt;/&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comparing Multiple Columns to Multiple lists a...</td>\n",
       "      <td>&lt;p&gt;I don't fully understand which output you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unable to change date format while loading exc...</td>\n",
       "      <td>&lt;p&gt;Using pd.to_datetime() allows for the use o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  igraph: get dict with nodes and corresponding ...   \n",
       "1  The first coordinates never work, but the foll...   \n",
       "2  Is there a performance deficit not splitting e...   \n",
       "3  Comparing Multiple Columns to Multiple lists a...   \n",
       "4  unable to change date format while loading exc...   \n",
       "\n",
       "                                         output_text  \n",
       "0  <p>You can recover the original networkx node ...  \n",
       "1  <p>Those first four lines:</p>\\n<pre><code>b =...  \n",
       "2  <p>Depends if your <code>function_one</code>/<...  \n",
       "3  <p>I don't fully understand which output you w...  \n",
       "4  <p>Using pd.to_datetime() allows for the use o...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the result to data frame \n",
    "stack_overflow_df = query_job.result().to_arrow().to_pandas()\n",
    "\n",
    "stack_overflow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding instruction for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Instructions\n",
    "\n",
    "- Instructions for LLMs have been shown to improve\n",
    "model performance and generalization to unseen tasks [(Google, 2022)](https://arxiv.org/pdf/2210.11416.pdf).\n",
    "- Wihtout the instruction, it is only question and answer. Model might not understand what to do.\n",
    "- With the instructions, the model gets a guideline as to what task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python. \\\n",
    "Answer it like you are a developer answering Stackoverflow questions.\n",
    "\n",
    "Stackoverflow question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new instruction, we will embed this instruction to the front of all the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow_df['input_text_instruction'] = INSTRUCTION_TEMPLATE + ' ' + stack_overflow_df['input_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the spliting ratio 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, eval = train_test_split(\n",
    "    stack_overflow_df, \n",
    "    test_size = 0.2,\n",
    "    random_state = 6701\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different datasets and flow \n",
    "- it's important to version data because it allows reproducibility, traceability, maintainability of machine learning models. \n",
    "- it's important to get the time stamp for the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "date = date.replace(':', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a `jsonl` file\n",
    "- Then name it as `tune_data_stack_overflow_python_qa-{date}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train dataset\n",
    "cols = ['input_text_instruction','output_text']\n",
    "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = f\"tune_data_stack_overflow_\\\n",
    "                            python_qa_{date}.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test dataste \n",
    "test_jsonl = eval[cols].to_json(orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_filename = f\"eval_data_stack_overflow_\\\n",
    "                            python_qa_{date}.jsonl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_data_filename, \"w\") as f: \n",
    "    f.write(test_jsonl)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
